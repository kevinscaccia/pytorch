{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model Training from Scratch Pytorch (Nano-GPT)\n",
    "\n",
    "- https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-13 07:38:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1,06M  --.-KB/s    in 0,04s   \n",
      "\n",
      "2023-12-13 07:38:24 (29,4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocess\n",
    "- Define vocabulary (Char-level)\n",
    "- Create tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary(65): ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "corpus = open('input.txt','r').read()\n",
    "chars = sorted(list(set(corpus)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f'Vocabulary({VOCAB_SIZE}):', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence \"hello scaccia!\" tokenized: [46, 43, 50, 50, 53, 1, 57, 41, 39, 41, 41, 47, 39, 2]\n",
      "Token list [46, 43, 50, 50, 53, 1, 58, 53, 49, 43, 52, 57] decoded: \"hello tokens\"\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, vocab_size):\n",
    "        self.char_to_int_map = {c:i for i,c in enumerate(chars)}\n",
    "        self.int_to_char_map = {v:k for k,v in self.char_to_int_map.items()} # reverse map\n",
    "        # self.oov = \n",
    "    def encode(self, txt):\n",
    "        tokens = [self.char_to_int_map[c] for c in txt]\n",
    "        return tokens\n",
    "    def decode(self, tokens):\n",
    "        chars = [self.int_to_char_map[t] for t in tokens]\n",
    "        return ''.join(chars) # list to str\n",
    "\n",
    "    \n",
    "    \n",
    "tokenizer = Tokenizer(VOCAB_SIZE)\n",
    "sentence = 'hello scaccia!'\n",
    "token_list = [46, 43, 50, 50, 53, 1, 58, 53, 49, 43, 52, 57]\n",
    "print(f'Sentence \"{sentence}\" tokenized: {tokenizer.encode(sentence)}')\n",
    "print(f'Token list {token_list} decoded: \"{tokenizer.decode(token_list)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Len: 1115394 tokens\n",
      "Train Len: 1003854 tokens\n",
      "Validation Len: 111540 tokens\n"
     ]
    }
   ],
   "source": [
    "corpus_tensor = torch.tensor(tokenizer.encode(corpus), dtype=torch.long)\n",
    "corpus_tensor[:10]\n",
    "print(f'Corpus Len: {len(corpus_tensor)} tokens')\n",
    "#\n",
    "split_point = int(len(corpus_tensor)*0.9) #90% for train\n",
    "train_data = corpus_tensor[:split_point]\n",
    "validation_data = corpus_tensor[split_point:]\n",
    "#\n",
    "print(f'Train Len: {len(train_data)} tokens')\n",
    "print(f'Validation Len: {len(validation_data)} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instance Sampling\n",
    "- block, context, chunk, sample,  qetc..\n",
    "-> maximum context length\n",
    "\n",
    "- O processo de criacao das instancias de treinamento supervisionadas (predict next word) amostra pequenos blocos de tokens do corpus original. Esses blocos são convertidos em varias instancias do tamanho 1 ate block_size-1. \n",
    "- a motivação é fazer com que o modelo seja acostumado a tomar entradas tao pequenas quanto 1 token e tao grandes quanto block_size. Para que no momento de inferencia ele esteja acostumado com sentencas de tamanhos variados.\n",
    "\n",
    "1. Amostra aleatoriamente da base um bloco de tokens de tamanho CONTEXT_LENGTH(tamanho maximo contexto )\n",
    "2. 33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This batch start indexes: tensor([35, 77, 72,  8])\n",
      "tensor([[35, 36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
      "        [77, 78, 79, 80, 81, 82, 83, 84, 85, 86],\n",
      "        [72, 73, 74, 75, 76, 77, 78, 79, 80, 81],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17]])\n",
      "tensor([[36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n",
      "        [78, 79, 80, 81, 82, 83, 84, 85, 86, 87],\n",
      "        [73, 74, 75, 76, 77, 78, 79, 80, 81, 82],\n",
      "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
      "Example (first sample of batch):\n",
      "For input [35] the target is 36\n",
      "For input [35 36] the target is 37\n",
      "For input [35 36 37] the target is 38\n",
      "For input [35 36 37 38] the target is 39\n",
      "For input [35 36 37 38 39] the target is 40\n",
      "For input [35 36 37 38 39 40] the target is 41\n",
      "For input [35 36 37 38 39 40 41] the target is 42\n",
      "For input [35 36 37 38 39 40 41 42] the target is 43\n",
      "For input [35 36 37 38 39 40 41 42 43] the target is 44\n",
      "For input [35 36 37 38 39 40 41 42 43 44] the target is 45\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(177)\n",
    "\n",
    "CONTEXT_LENGTH = 10\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def get_batch(data, batch_size, context_len):\n",
    "    # generate start index of all batches\n",
    "    start_ixs = torch.randint(low=0, high=len(data)-context_len, size=(batch_size,))\n",
    "    # all batches at once\n",
    "    print(f'This batch start indexes: {start_ixs}')\n",
    "    # print('first batch:', data[start_ixs[0]:start_ixs[0]+context_len])\n",
    "    batch_x = torch.vstack([data[i:i+context_len] for i in start_ixs]) # stack each sample in a row\n",
    "    batch_y = torch.vstack([data[i+1:i+context_len+1] for i in start_ixs]) # aligned\n",
    "\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "X, Y = get_batch(torch.tensor(list(range(0,100))), BATCH_SIZE, CONTEXT_LENGTH)\n",
    "print(X)\n",
    "print(Y)\n",
    "# each row in the batch is in really 10 instances (varying the context len)\n",
    "print('Example (first sample of batch):')\n",
    "for t in range(CONTEXT_LENGTH): # in time dimension(sequence)\n",
    "    _x = X[0][:t+1].numpy()\n",
    "    _y = Y[0][t].numpy()\n",
    "    print(f'For input {_x} the target is {_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This batch start indexes: tensor([958983, 873237, 212788, 708270])\n"
     ]
    }
   ],
   "source": [
    "train_batch_x, train_batch_y = get_batch(train_data, BATCH_SIZE, CONTEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(65, 2)\n",
      "tensor([[ 1, 39, 52, 42,  1, 57, 43, 55, 59, 43]]) torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3921,  0.7385],\n",
       "         [-0.0585, -0.2032],\n",
       "         [ 1.1596, -0.4916],\n",
       "         [ 0.4894, -1.4209],\n",
       "         [ 0.3921,  0.7385],\n",
       "         [ 0.5515, -1.0497],\n",
       "         [ 0.4411, -1.2156],\n",
       "         [ 0.8534, -1.0156],\n",
       "         [-2.0627, -1.0285],\n",
       "         [ 0.4411, -1.2156]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(VOCAB_SIZE, embedding_dim=2)\n",
    "print(embedding_layer)\n",
    "_x = train_batch_x[0:1]\n",
    "print(_x, _x.shape)\n",
    "# convert each token in a float vector (embedding vector)\n",
    "embedding_layer(_x) # each token has a associated embedding (like a lookup table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10]) torch.Size([4, 10, 65])\n",
      "torch.Size([4, 10, 65])\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.embedding_layer(idx)\n",
    "        # (batch_size, context_len) ((batch_size, context_len, embedding_dim)\n",
    "        print(idx.shape, logits.shape) \n",
    "        return logits\n",
    "\n",
    "# if we pass vocab_size as embedding_dim, we have that each word in input\n",
    "# has am embedding that representes the probability of every other word in vocabulary(vocab size)\n",
    "# So we call logits\n",
    "model = BigramLanguageModel(VOCAB_SIZE, VOCAB_SIZE) \n",
    "pred_y = model(train_batch_x, train_batch_y)\n",
    "print(pred_y.shape) # for each sentence in the batch, we have the logits of each word (next word prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
