{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model Training from Scratch Pytorch (Nano-GPT)\n",
    "\n",
    "- https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocess\n",
    "- Define vocabulary (Char-level)\n",
    "- Create tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary(65): ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "corpus = open('input.txt','r').read()\n",
    "chars = sorted(list(set(corpus)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f'Vocabulary({VOCAB_SIZE}):', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence \"hello scaccia!\" tokenized: [46, 43, 50, 50, 53, 1, 57, 41, 39, 41, 41, 47, 39, 2]\n",
      "Token list [46, 43, 50, 50, 53, 1, 58, 53, 49, 43, 52, 57] decoded: \"hello tokens\"\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.char_to_int_map = {c:i for i,c in enumerate(chars)}\n",
    "        self.int_to_char_map = {i:c for c,i in self.char_to_int_map.items()} # reverse map\n",
    "        # self.oov = \n",
    "    def encode(self, txt):\n",
    "        tokens = [self.char_to_int_map[c] for c in txt]\n",
    "        return tokens\n",
    "    def decode(self, tokens):\n",
    "        chars = [self.int_to_char_map[t] for t in tokens]\n",
    "        return ''.join(chars) # list to str\n",
    "\n",
    "    \n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "sentence = 'hello scaccia!'\n",
    "token_list = [46, 43, 50, 50, 53, 1, 58, 53, 49, 43, 52, 57]\n",
    "print(f'Sentence \"{sentence}\" tokenized: {tokenizer.encode(sentence)}')\n",
    "print(f'Token list {token_list} decoded: \"{tokenizer.decode(token_list)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "Corpus Len: 1115394 tokens\n",
      "Train Len: 1003854 tokens\n",
      "Validation Len: 111540 tokens\n"
     ]
    }
   ],
   "source": [
    "corpus_tensor = torch.tensor(tokenizer.encode(corpus), dtype=torch.long)\n",
    "print(corpus_tensor[:10])\n",
    "print(f'Corpus Len: {len(corpus_tensor)} tokens')\n",
    "#\n",
    "split_point = int(len(corpus_tensor)*0.9) #90% for train\n",
    "train_data = corpus_tensor[:split_point]\n",
    "validation_data = corpus_tensor[split_point:]\n",
    "#\n",
    "print(f'Train Len: {len(train_data)} tokens')\n",
    "print(f'Validation Len: {len(validation_data)} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instance Sampling\n",
    "- block, context, chunk, sample,  qetc..\n",
    "-> maximum context length\n",
    "\n",
    "- O processo de criacao das instancias de treinamento supervisionadas (predict next word) amostra pequenos blocos de tokens do corpus original. Esses blocos são convertidos em varias instancias do tamanho 1 ate block_size-1. \n",
    "- a motivação é fazer com que o modelo seja acostumado a tomar entradas tao pequenas quanto 1 token e tao grandes quanto block_size. Para que no momento de inferencia ele esteja acostumado com sentencas de tamanhos variados.\n",
    "\n",
    "1. Amostra aleatoriamente da base um bloco de tokens de tamanho CONTEXT_LENGTH(tamanho maximo contexto )\n",
    "2. 33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35, 36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
      "        [77, 78, 79, 80, 81, 82, 83, 84, 85, 86],\n",
      "        [72, 73, 74, 75, 76, 77, 78, 79, 80, 81],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17]])\n",
      "tensor([[36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n",
      "        [78, 79, 80, 81, 82, 83, 84, 85, 86, 87],\n",
      "        [73, 74, 75, 76, 77, 78, 79, 80, 81, 82],\n",
      "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
      "Example (first sample of batch):\n",
      "For input [35] the target is 36\n",
      "For input [35 36] the target is 37\n",
      "For input [35 36 37] the target is 38\n",
      "For input [35 36 37 38] the target is 39\n",
      "For input [35 36 37 38 39] the target is 40\n",
      "For input [35 36 37 38 39 40] the target is 41\n",
      "For input [35 36 37 38 39 40 41] the target is 42\n",
      "For input [35 36 37 38 39 40 41 42] the target is 43\n",
      "For input [35 36 37 38 39 40 41 42 43] the target is 44\n",
      "For input [35 36 37 38 39 40 41 42 43 44] the target is 45\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(177)\n",
    "\n",
    "CONTEXT_LENGTH = 10\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def get_batch(data, batch_size, context_len, verbose=False):\n",
    "    # generate start index of all batches\n",
    "    start_ixs = torch.randint(low=0, high=len(data)-context_len, size=(batch_size,))\n",
    "    # all batches at once\n",
    "    if verbose: print(f'This batch start indexes: {start_ixs}')\n",
    "    # print('first batch:', data[start_ixs[0]:start_ixs[0]+context_len])\n",
    "    batch_x = torch.vstack([data[i:i+context_len] for i in start_ixs]) # stack each sample in a row\n",
    "    batch_y = torch.vstack([data[i+1:i+context_len+1] for i in start_ixs]) # aligned\n",
    "\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "X, Y = get_batch(torch.tensor(list(range(0,100))), BATCH_SIZE, CONTEXT_LENGTH)\n",
    "print(X)\n",
    "print(Y)\n",
    "# each row in the batch is in really 10 instances (varying the context len)\n",
    "print('Example (first sample of batch):')\n",
    "for t in range(CONTEXT_LENGTH): # in time dimension(sequence)\n",
    "    _x = X[0][:t+1].numpy()\n",
    "    _y = Y[0][t].numpy()\n",
    "    print(f'For input {_x} the target is {_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BigramLanguageModel\n",
    "- For a given token t we have all the probabilities, once per token, of all others to be the next token (t+1)\n",
    "- the embedding layer acts a simple dense vocab_size x vocab_size matrix of weights.\n",
    "- Th probability for the next token use only the information of current token, no context.\n",
    "- Logits are the raw probability of each token to be the next (scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = get_batch(train_data, BATCH_SIZE, CONTEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 loss: 4.7051\n",
      "Epoch 101/10000 loss: 4.5502\n",
      "Epoch 201/10000 loss: 4.4961\n",
      "Epoch 301/10000 loss: 4.3012\n",
      "Epoch 401/10000 loss: 4.2813\n",
      "Epoch 501/10000 loss: 4.0629\n",
      "Epoch 601/10000 loss: 4.0314\n",
      "Epoch 701/10000 loss: 3.9738\n",
      "Epoch 801/10000 loss: 3.8933\n",
      "Epoch 901/10000 loss: 3.8096\n",
      "Epoch 1001/10000 loss: 3.7367\n",
      "Epoch 1101/10000 loss: 3.6010\n",
      "Epoch 1201/10000 loss: 3.4686\n",
      "Epoch 1301/10000 loss: 3.4420\n",
      "Epoch 1401/10000 loss: 3.4132\n",
      "Epoch 1501/10000 loss: 3.4171\n",
      "Epoch 1601/10000 loss: 3.2906\n",
      "Epoch 1701/10000 loss: 3.2640\n",
      "Epoch 1801/10000 loss: 3.1036\n",
      "Epoch 1901/10000 loss: 3.1852\n",
      "Epoch 2001/10000 loss: 3.1669\n",
      "Epoch 2101/10000 loss: 3.0120\n",
      "Epoch 2201/10000 loss: 3.0428\n",
      "Epoch 2301/10000 loss: 3.0109\n",
      "Epoch 2401/10000 loss: 2.8937\n",
      "Epoch 2501/10000 loss: 2.8562\n",
      "Epoch 2601/10000 loss: 2.9875\n",
      "Epoch 2701/10000 loss: 2.8274\n",
      "Epoch 2801/10000 loss: 2.7703\n",
      "Epoch 2901/10000 loss: 2.7320\n",
      "Epoch 3001/10000 loss: 2.8193\n",
      "Epoch 3101/10000 loss: 2.8193\n",
      "Epoch 3201/10000 loss: 2.7148\n",
      "Epoch 3301/10000 loss: 2.7615\n",
      "Epoch 3401/10000 loss: 2.7072\n",
      "Epoch 3501/10000 loss: 2.7013\n",
      "Epoch 3601/10000 loss: 2.6953\n",
      "Epoch 3701/10000 loss: 2.6841\n",
      "Epoch 3801/10000 loss: 2.5883\n",
      "Epoch 3901/10000 loss: 2.7001\n",
      "Epoch 4001/10000 loss: 2.6329\n",
      "Epoch 4101/10000 loss: 2.4550\n",
      "Epoch 4201/10000 loss: 2.6932\n",
      "Epoch 4301/10000 loss: 2.6295\n",
      "Epoch 4401/10000 loss: 2.5558\n",
      "Epoch 4501/10000 loss: 2.5491\n",
      "Epoch 4601/10000 loss: 2.5619\n",
      "Epoch 4701/10000 loss: 2.6356\n",
      "Epoch 4801/10000 loss: 2.6589\n",
      "Epoch 4901/10000 loss: 2.4575\n",
      "Epoch 5001/10000 loss: 2.5129\n",
      "Epoch 5101/10000 loss: 2.5671\n",
      "Epoch 5201/10000 loss: 2.5413\n",
      "Epoch 5301/10000 loss: 2.6510\n",
      "Epoch 5401/10000 loss: 2.6352\n",
      "Epoch 5501/10000 loss: 2.4364\n",
      "Epoch 5601/10000 loss: 2.6338\n",
      "Epoch 5701/10000 loss: 2.4744\n",
      "Epoch 5801/10000 loss: 2.4867\n",
      "Epoch 5901/10000 loss: 2.3708\n",
      "Epoch 6001/10000 loss: 2.5530\n",
      "Epoch 6101/10000 loss: 2.4514\n",
      "Epoch 6201/10000 loss: 2.4315\n",
      "Epoch 6301/10000 loss: 2.4940\n",
      "Epoch 6401/10000 loss: 2.5822\n",
      "Epoch 6501/10000 loss: 2.3949\n",
      "Epoch 6601/10000 loss: 2.4438\n",
      "Epoch 6701/10000 loss: 2.3867\n",
      "Epoch 6801/10000 loss: 2.4996\n",
      "Epoch 6901/10000 loss: 2.5530\n",
      "Epoch 7001/10000 loss: 2.4261\n",
      "Epoch 7101/10000 loss: 2.4779\n",
      "Epoch 7201/10000 loss: 2.4916\n",
      "Epoch 7301/10000 loss: 2.5423\n",
      "Epoch 7401/10000 loss: 2.4358\n",
      "Epoch 7501/10000 loss: 2.4573\n",
      "Epoch 7601/10000 loss: 2.5028\n",
      "Epoch 7701/10000 loss: 2.4529\n",
      "Epoch 7801/10000 loss: 2.3507\n",
      "Epoch 7901/10000 loss: 2.5146\n",
      "Epoch 8001/10000 loss: 2.4542\n",
      "Epoch 8101/10000 loss: 2.5842\n",
      "Epoch 8201/10000 loss: 2.3896\n",
      "Epoch 8301/10000 loss: 2.3931\n",
      "Epoch 8401/10000 loss: 2.4698\n",
      "Epoch 8501/10000 loss: 2.5150\n",
      "Epoch 8601/10000 loss: 2.5477\n",
      "Epoch 8701/10000 loss: 2.4888\n",
      "Epoch 8801/10000 loss: 2.6376\n",
      "Epoch 8901/10000 loss: 2.3621\n",
      "Epoch 9001/10000 loss: 2.4754\n",
      "Epoch 9101/10000 loss: 2.5367\n",
      "Epoch 9201/10000 loss: 2.4879\n",
      "Epoch 9301/10000 loss: 2.3523\n",
      "Epoch 9401/10000 loss: 2.5556\n",
      "Epoch 9501/10000 loss: 2.4193\n",
      "Epoch 9601/10000 loss: 2.4844\n",
      "Epoch 9701/10000 loss: 2.4952\n",
      "Epoch 9801/10000 loss: 2.5232\n",
      "Epoch 9901/10000 loss: 2.5145\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lr=1e-3):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        logits = self.embedding_layer(idx)\n",
    "        # (batch_size, context_len) ((batch_size, context_len, embedding_dim)\n",
    "        # print(idx.shape, logits.shape) \n",
    "        return logits\n",
    "    \n",
    "    def get_loss(self, idx, targets, verbose=False):\n",
    "        logits = self.forward(idx)\n",
    "        B, T, C = logits.shape # Channel dimention: embedding dim\n",
    "        if verbose: print(idx.shape, '-->', logits.shape, '-->',logits.view(B*T, -1).shape,)\n",
    "        if verbose: print(targets.shape, '-->', targets.view(-1).shape,)\n",
    "        # in the first dimension we have all batches concatened, and all its instances\n",
    "        # in the second dimension we have the probabilities of each input\n",
    "        logits = logits.view(B*T, -1)\n",
    "        targets = targets.view(-1) # all the indexes, in one row\n",
    "\n",
    "        if verbose: print(f'One example: {logits[0,:3]}..... : target: {targets[0]}')\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, n_tokens):\n",
    "        # for all batch, parallel generate next tokens n times\n",
    "        for i in range(n_tokens):\n",
    "            logits = self(idx)\n",
    "            # focus on the last time step (because the forward predicts for all possible input len)\n",
    "            logits = logits[:, -1, :] # (batch_size, all tokens probability) - (B,C)\n",
    "            # apply softmax to get probability of the next token \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            # sample based in the probability. (dont use only the most probable token)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) only one token per batch\n",
    "            # print(idx.shape, idx_next.shape)\n",
    "            # concatenate in the history of tokens\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    def train(self, data, epochs, batch_size, verbose=False):\n",
    "        for epoch_i in range(epochs):\n",
    "            # random batch\n",
    "            batch_x, batch_y = get_batch(data, batch_size, CONTEXT_LENGTH)\n",
    "            if verbose: print('batch_x:', batch_x.shape, 'batch_y:', batch_y.shape)\n",
    "            # forward and loss \n",
    "            loss = self.get_loss(batch_x, batch_y) \n",
    "            self.optimizer.zero_grad(set_to_none=True) # already connected to the model weights\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if epoch_i % 100 == 0:\n",
    "                print(f'Epoch {epoch_i+1}/{epochs} loss: {loss.item():.4f}')\n",
    "        \n",
    "\n",
    "model = BigramLanguageModel(VOCAB_SIZE, VOCAB_SIZE) \n",
    "model = model.to('cuda')\n",
    "train_data = train_data.to('cuda')\n",
    "\n",
    "model.train(train_data, batch_size=32, epochs=10_000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Had d y touls my praithits en g ind m yofr mereratold matramenk, toung, an lll t pls wighyosthive\n",
      "\n",
      "Deache theco. benglosp, is p, JUUSO,\n",
      "CE pllinowikedrThel IS:\n",
      "Foupaje h\n",
      "Wirit n; thorouisore ber:\n",
      "TVIshaise wirulves w, ayod llen?\n",
      "THeldghane hevengis,\n",
      "\n",
      "My tsx?\n",
      "'stat.\n",
      "Fin thin fan's; meaplllt, flloulam utsiny hnghe hirr blin, r ne us ak, ng\n",
      "\n",
      "\n",
      "T;\n",
      "Cofrthal; malous he CY hand chejed isemulouanthys, fe pthenonasand; thofar isonthy HAyoswhira g th'Thankbor og t m'sswnk.\n",
      "TELa bu, ml sure:\n",
      "Whesherartors ug\n",
      "\n",
      "\n",
      "An a he car, t Y gf 'daworof nthenthy icilleame gat.\n",
      "CHomyo mug y par me:\n",
      "cterore towesnere mblaghouis e,\n",
      "MORI gikeacit r:\n",
      "I cl.\n",
      "\n",
      "\n",
      "NAnve,x'ed cr llme thoted thoous th ar silerivers hallatoiowise ag hin tt ampoug me inge lyr senelereratompr, we Yovim I andy.\n",
      "\n",
      "R:\n",
      "\n",
      "CUCONo.\n",
      "\n",
      "INTouk hin PED th trole betow ll d t chotarg.\n",
      "Whirale d brd r\n",
      "'ed w; p je t,\n",
      "Matheather he p?\n",
      "II:\n",
      "Gour pe ngmmeiid mbe bu ye ENG k'\n",
      "Wh ICl ff r hertheroue fodan as Mate teilf! tw w s.\n",
      "g d arcy thent usin, at\n",
      "Ithal ter:\n",
      "\n",
      "A:\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "first_seed_token = torch.tensor([[0]], dtype=torch.long).to('cuda')  #torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "generated = model.generate(first_seed_token, n_tokens=1000).tolist()[0]\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if we pass vocab_size as embedding_dim, we have that each word in input\n",
    "# has am embedding that representes the probability of every other word in vocabulary(vocab size)\n",
    "# So we call logits\n",
    "model = BigramLanguageModel(VOCAB_SIZE, VOCAB_SIZE) \n",
    "pred_y = model(train_batch_x)\n",
    "# print(pred_y.shape) # for each sentence in the batch, we have the logits of each word (next word prediction)\n",
    "# print(train_batch_y.shape)\n",
    "# model.get_loss(train_batch_x, train_batch_y)\n",
    "model.generate(train_batch_x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if we pass vocab_size as embedding_dim, we have that each word in input\n",
    "# has am embedding that representes the probability of every other word in vocabulary(vocab size)\n",
    "# So we call logits\n",
    "pred_y = model(train_batch_x)\n",
    "# print(pred_y.shape) # for each sentence in the batch, we have the logits of each word (next word prediction)\n",
    "# print(train_batch_y.shape)\n",
    "model.generate(train_batch_x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10]) --> torch.Size([4, 10, 65]) --> torch.Size([40, 65])\n",
      "torch.Size([4, 10]) --> torch.Size([40])\n",
      "One example: tensor([-0.4873,  0.1745,  2.0194], grad_fn=<SliceBackward0>)..... : target: 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.7219, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_loss(train_batch_x, train_batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 60, 12, 55, 29, 28, 22, 57, 1, 48, 22, 45, 0, 8, 41, 30, 4, 13, 45, 23, 37, 28, 48, 37, 58, 56, 45, 57, 3, 28, 48, 64, 8, 29, 39, 34, 41, 25, 9, 21, 59, 25, 42, 15, 18, 34, 4, 41, 61, 30, 22, 42, 8, 23, 5, 22, 46, 37, 34, 23, 37, 28, 48, 46, 46, 46, 63, 57, 15, 30, 2, 49, 9, 42, 0, 61, 33, 60, 51, 2, 62, 37, 58, 13, 64, 61, 6, 13, 56, 45, 0, 39, 19, 25, 25, 56, 29, 60, 64, 5, 26]\n",
      "\n",
      "v?qQPJs jJg\n",
      ".cR&AgKYPjYtrgs$Pjz.QaVcM3IuMdCFV&cwRJd.K'JhYVKYPjhhhysCR!k3d\n",
      "wUvm!xYtAzw,Arg\n",
      "aGMMrQvz'N\n"
     ]
    }
   ],
   "source": [
    "first_seed_token = torch.tensor([[0]], dtype=torch.long)  #torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "generated = model.generate(first_seed_token, n_tokens=100).tolist()[0]\n",
    "print(generated)\n",
    "print(tokenizer.decode(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 65])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[1,1,1,1,1,1,1,3]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = nn.Embedding(VOCAB_SIZE, embedding_dim=2)\n",
    "# print(embedding_layer)\n",
    "# _x = train_batch_x[0:1]\n",
    "# print(_x, _x.shape)\n",
    "# # convert each token in a float vector (embedding vector)\n",
    "# embedding_layer(_x) # each token has a associated embedding (like a lookup table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
